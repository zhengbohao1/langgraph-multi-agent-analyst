# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. èŠ‚ç‚¹å‡½æ•°ï¼šç»Ÿè®¡åˆ†æ
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import json


from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.types import Command


from app.agents.data_analyst_agent.state import AnalystState
from app.models.LLM_MODEL import ModelInstances
from app.prompts.data_analyst_agent_prompt import STATISTICAL_ANALYSIS_PROMPT, DATA_ANALYST_AGENT_SYSTEM_PROMPT


async def statistical_analysis_node(state: AnalystState) :
    """ç»Ÿè®¡åˆ†æèŠ‚ç‚¹"""
    print("ğŸ” æ‰§è¡Œç»Ÿè®¡åˆ†æèŠ‚ç‚¹...")

    input_data = state["input_data"]

    # å‡†å¤‡æç¤ºè¯
    prompt = STATISTICAL_ANALYSIS_PROMPT.format(
        source=input_data.source,
        path=input_data.path,
        columns=", ".join(input_data.columns),
        row_count=input_data.row_count,
        reflection="" if not state["stat_reflection"] else state["stat_reflection"]
    )

    # å°†æ•°æ®è¡Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œæ–¹ä¾¿ LLM ç†è§£
    sample_rows = input_data.all_rows[:100]  # é™åˆ¶æ ·æœ¬æ•°é‡ï¼Œé¿å… token è¿‡å¤š
    data_sample = json.dumps(sample_rows, ensure_ascii=False, indent=2)
    prompt += f"\n\næ•°æ®æ ·æœ¬ï¼ˆå‰100è¡Œï¼‰ï¼š\n{data_sample}"
    # print(f"æ£€æŸ¥æç¤ºè¯ï¼š{prompt}")

    # è°ƒç”¨ LLM
    messages = [
        SystemMessage(content=DATA_ANALYST_AGENT_SYSTEM_PROMPT),
        HumanMessage(content=prompt)
    ]

    llm = ModelInstances.analyst_llm
    response =await llm.ainvoke(messages)

    #ä¿å­˜mdæ ¼å¼è¾“å‡º

    return Command(update={"statistical_result":response.content})