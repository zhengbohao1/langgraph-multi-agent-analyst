# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 7. èŠ‚ç‚¹å‡½æ•°ï¼šåæ€èŠ‚ç‚¹ï¼ˆé€šç”¨ï¼‰
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import json

from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.types import Command

from app.agents.data_analyst_agent.format import ReflectionResult
from app.agents.data_analyst_agent.state import AnalystState
from app.models.LLM_MODEL import ModelInstances
from app.prompts.data_analyst_agent_prompt import DATA_ANALYST_AGENT_SYSTEM_PROMPT, REFLECTION_PROMPT, \
    ANOMALY_REFLECTION_PROMPT, TREND_REFLECTION_PROMPT, STAT_REFLECTION_PROMPT


async def stat_reflection_node(state: AnalystState) :
    """åæ€ç»Ÿè®¡åˆ†ææŠ¥å‘Šçš„è´¨é‡"""
    print("ğŸ¤” æ‰§è¡Œç»Ÿè®¡åˆ†æåæ€èŠ‚ç‚¹...")


    analysis_summary = json.dumps(state.get("statistical_result", {}), ensure_ascii=False)
    node_name = "ç»Ÿè®¡åˆ†æ"
    state["stat_iteration_count"] += 1
    iteration_count = state["stat_iteration_count"]

    prompt = STAT_REFLECTION_PROMPT.format(
        node_name=node_name,
        analysis_summary=analysis_summary[:3000]  # å¯é€‚å½“æ”¾å®½é•¿åº¦ï¼Œå› ä¸º Markdown å¯èƒ½è¾ƒé•¿
    )

    messages = [
        SystemMessage(content=DATA_ANALYST_AGENT_SYSTEM_PROMPT),
        HumanMessage(content=prompt)
    ]

    llm = ModelInstances.analyst_llm.with_structured_output(ReflectionResult)
    response = await llm.ainvoke(messages)

    reflection_result = response.model_dump()
    # state["stat_reflection"] = reflection_result

    print(f"ç»Ÿè®¡åˆ†æåæ€ - å½“å‰æ­¥æ•°ï¼š{iteration_count}ï¼Œç»“æœï¼š{reflection_result}")

    return Command(update={"stat_iteration_count":iteration_count,"stat_reflection":reflection_result})
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åæ€èŠ‚ç‚¹ï¼šè¶‹åŠ¿é¢„æµ‹ä¸“ç”¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def trend_reflection_node(state: AnalystState):
    """åæ€è¶‹åŠ¿é¢„æµ‹æŠ¥å‘Šçš„è´¨é‡"""
    print("ğŸ¤” æ‰§è¡Œè¶‹åŠ¿é¢„æµ‹åæ€èŠ‚ç‚¹...")

    analysis_summary = json.dumps(state.get("trend_result", {}), ensure_ascii=False)
    node_name = "è¶‹åŠ¿é¢„æµ‹"
    state["trend_iteration_count"] += 1
    iteration_count = state["trend_iteration_count"]

    prompt = TREND_REFLECTION_PROMPT.format(
        node_name=node_name,
        analysis_summary=analysis_summary[:3000]
    )

    messages = [
        SystemMessage(content=DATA_ANALYST_AGENT_SYSTEM_PROMPT),
        HumanMessage(content=prompt)
    ]

    llm = ModelInstances.analyst_llm.with_structured_output(ReflectionResult)
    response = await llm.ainvoke(messages)

    reflection_result = response.model_dump()
    state["trend_reflection"] = reflection_result

    print(f"è¶‹åŠ¿é¢„æµ‹åæ€ - å½“å‰æ­¥æ•°ï¼š{iteration_count}ï¼Œç»“æœï¼š{reflection_result}")

    return Command(update={"trend_iteration_count":iteration_count,"trend_reflection":reflection_result})


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# åæ€èŠ‚ç‚¹ï¼šå¼‚å¸¸æ£€æµ‹ä¸“ç”¨
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
async def anomaly_reflection_node(state: AnalystState) :
    """åæ€å¼‚å¸¸æ£€æµ‹æŠ¥å‘Šçš„è´¨é‡"""
    print("ğŸ¤” æ‰§è¡Œå¼‚å¸¸æ£€æµ‹åæ€èŠ‚ç‚¹...")

    analysis_summary = json.dumps(state.get("anomaly_result", {}), ensure_ascii=False)
    node_name = "å¼‚å¸¸æ£€æµ‹"
    state["anomaly_iteration_count"] += 1
    iteration_count = state["anomaly_iteration_count"]

    prompt = ANOMALY_REFLECTION_PROMPT.format(
        node_name=node_name,
        analysis_summary=analysis_summary[:3000]
    )

    messages = [
        SystemMessage(content=DATA_ANALYST_AGENT_SYSTEM_PROMPT),
        HumanMessage(content=prompt)
    ]

    llm = ModelInstances.analyst_llm.with_structured_output(ReflectionResult)
    response = await llm.ainvoke(messages)

    reflection_result = response.model_dump()
    state["anomaly_reflection"] = reflection_result

    print(f"å¼‚å¸¸æ£€æµ‹åæ€ - å½“å‰æ­¥æ•°ï¼š{iteration_count}ï¼Œç»“æœï¼š{reflection_result}")

    return Command(update={"anomaly_iteration_count":iteration_count,"anomaly_reflection":reflection_result})