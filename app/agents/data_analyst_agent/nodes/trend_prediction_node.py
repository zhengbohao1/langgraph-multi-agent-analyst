# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. èŠ‚ç‚¹å‡½æ•°ï¼šè¶‹åŠ¿é¢„æµ‹
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
import json

from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.types import Command

from app.agents.data_analyst_agent.format import TrendPredictionResult
from app.agents.data_analyst_agent.state import AnalystState
from app.models.LLM_MODEL import ModelInstances
from app.prompts.data_analyst_agent_prompt import TREND_PREDICTION_PROMPT, DATA_ANALYST_AGENT_SYSTEM_PROMPT


async def trend_prediction_node(state: AnalystState) :
    """è¶‹åŠ¿é¢„æµ‹èŠ‚ç‚¹"""
    print("ğŸ“ˆ æ‰§è¡Œè¶‹åŠ¿é¢„æµ‹èŠ‚ç‚¹...")

    input_data = state["input_data"]

    # å‡†å¤‡æç¤ºè¯
    prompt = TREND_PREDICTION_PROMPT.format(
        source=input_data.source,
        path=input_data.path,
        columns=", ".join(input_data.columns),
        row_count=input_data.row_count,
        all_rows=input_data.all_rows,
        reflection="" if not state["trend_reflection"] else state["trend_reflection"]
    )

    # å°†æ•°æ®è¡Œè½¬æ¢ä¸ºå­—ç¬¦ä¸²
    sample_rows = input_data.all_rows[:100]
    data_sample = json.dumps(sample_rows, ensure_ascii=False, indent=2)
    prompt += f"\n\næ•°æ®æ ·æœ¬ï¼ˆå‰100è¡Œï¼‰ï¼š\n{data_sample}"

    # è°ƒç”¨ LLM
    messages = [
        SystemMessage(content=DATA_ANALYST_AGENT_SYSTEM_PROMPT),
        HumanMessage(content=prompt)
    ]

    llm = ModelInstances.analyst_llm
    response = await llm.ainvoke(messages)


    return Command(update={"trend_result":response.content})